{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7c44834-4b7b-45b0-a0a1-8c0f14996431",
   "metadata": {},
   "source": [
    "# Import Skydrifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcfcedb-b8dc-4834-8e2f-da7e5da8ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\bonda\\Documents\\Bondan\\TEWS Testing\\skydrifter\")\n",
    "import skydrifter as sd\n",
    "from skydrifter.Utils.partisan import *\n",
    "from skydrifter.Utils.nonpartisan import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52deed1-24ea-49b5-8c2a-e255d9c9027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skydrifter.PlotModeCollection.plotting import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be5a68-acf4-404e-9650-82e463bd94b1",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a05a5a-aeaf-49c5-b015-b4fc4bb24512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seisbench.models as sbm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32c06d-7e31-45a4-b046-6f4d5488468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seisbench\n",
    "seisbench.use_backup_repository()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4691462-1f96-4e73-81f3-995528f6cc36",
   "metadata": {},
   "source": [
    "# Domestic Funtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d6f10c-43c2-47ec-a657-226c8ddfaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nan_with_data_index(tensor):\n",
    "    nan_mask = torch.isnan(tensor)\n",
    "    contains_nan = nan_mask.any().item()\n",
    "    nan_indices = nan_mask.nonzero(as_tuple=True) if contains_nan else tuple()\n",
    "    data_indices = nan_indices[0].unique().tolist() if contains_nan else []\n",
    "    return data_indices\n",
    "\n",
    "def remove_indices(lst, indices_to_remove):\n",
    "    indices_to_remove = set(indices_to_remove)\n",
    "    return [item for idx, item in enumerate(lst) if idx not in indices_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05245ac5-e7e7-4483-97a3-d39c1265acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(y_pred,y_true,series,label_position):\n",
    "    #\n",
    "    pred_p_index = torch.argmax(y_pred[0,label_position['P']]).numpy()\n",
    "    pred_s_index = torch.argmax(y_pred[0,label_position['S']]).numpy()\n",
    "    pred_prob_p = y_pred[0,label_position['P'],pred_p_index].numpy()\n",
    "    pred_prob_s = y_pred[0,label_position['S'],pred_s_index].numpy()\n",
    "    pred_arrival_p = op.UTCDateTime(series.Starttime) + ((1/series.sampling_rate)*pred_p_index)\n",
    "    pred_arrival_s = op.UTCDateTime(series.Starttime) + ((1/series.sampling_rate)*pred_s_index)\n",
    "    #\n",
    "    true_p_index = torch.argmax(y_true[0,label_position['P']]).numpy()\n",
    "    true_s_index = torch.argmax(y_true[0,label_position['S']]).numpy()\n",
    "    true_prob_p = y_true[0,label_position['P'],true_p_index].numpy()\n",
    "    true_prob_s = y_true[0,label_position['S'],true_s_index].numpy()\n",
    "    true_arrival_p = op.UTCDateTime(series.Starttime) + ((1/series.sampling_rate)*true_p_index)\n",
    "    true_arrival_s = op.UTCDateTime(series.Starttime) + ((1/series.sampling_rate)*true_s_index)\n",
    "    #\n",
    "    residual_p = abs(true_arrival_p - pred_arrival_p)\n",
    "    residual_s = abs(true_arrival_s - pred_arrival_s)\n",
    "    output = {\n",
    "        'true_p_index': true_p_index,\n",
    "        'true_s_index': true_s_index,\n",
    "        'true_prob_p': true_prob_p,\n",
    "        'true_prob_s': true_prob_s,\n",
    "        'true_arrival_p': true_arrival_p,\n",
    "        'true_arrival_s': true_arrival_s,\n",
    "        'pred_p_index': pred_p_index,\n",
    "        'pred_s_index': pred_s_index,\n",
    "        'pred_prob_p': pred_prob_p,\n",
    "        'pred_prob_s': pred_prob_s,\n",
    "        'pred_arrival_p': pred_arrival_p,\n",
    "        'pred_arrival_s': pred_arrival_s,\n",
    "        'residual_p': residual_p,\n",
    "        'residual_s': residual_s,\n",
    "    }\n",
    "    return output\n",
    "\n",
    "def confusion(y_pred,y_true,label_position,true_threshold=0.4,pred_threshold=0.6):\n",
    "    #\n",
    "    pred_p_arr = y_pred[0,label_position['P']].numpy()\n",
    "    pred_s_arr = y_pred[0,label_position['S']].numpy()\n",
    "    #\n",
    "    true_p_arr = y_true[0,label_position['P']].numpy()\n",
    "    true_s_arr = y_true[0,label_position['S']].numpy()\n",
    "    #\n",
    "    pred_p_binary_arr = (pred_p_arr > pred_threshold).astype(int)\n",
    "    pred_s_binary_arr = (pred_s_arr > pred_threshold).astype(int)\n",
    "    #\n",
    "    true_p_binary_arr = (true_p_arr > true_threshold).astype(int)\n",
    "    true_s_binary_arr = (true_s_arr > true_threshold).astype(int)\n",
    "    #\n",
    "    TP_p = np.sum((true_p_binary_arr == 1) & (pred_p_binary_arr == 1))  \n",
    "    TN_p = np.sum((true_p_binary_arr == 0) & (pred_p_binary_arr == 0))  \n",
    "    FP_p = np.sum((true_p_binary_arr == 0) & (pred_p_binary_arr == 1)) \n",
    "    FN_p = np.sum((true_p_binary_arr == 1) & (pred_p_binary_arr == 0))\n",
    "    #\n",
    "    TP_s = np.sum((true_s_binary_arr == 1) & (pred_s_binary_arr == 1))  \n",
    "    TN_s = np.sum((true_s_binary_arr == 0) & (pred_s_binary_arr == 0))  \n",
    "    FP_s = np.sum((true_s_binary_arr == 0) & (pred_s_binary_arr == 1)) \n",
    "    FN_s = np.sum((true_s_binary_arr == 1) & (pred_s_binary_arr == 0))\n",
    "    #\n",
    "    output = {\n",
    "        'TP_p': TP_p,  \n",
    "        'TN_p': TN_p,  \n",
    "        'FP_p': FP_p, \n",
    "        'FN_p': FN_p,\n",
    "        'TP_s': TP_s,  \n",
    "        'TN_s': TN_s,  \n",
    "        'FP_s': FP_s, \n",
    "        'FN_s': FN_s\n",
    "    }\n",
    "    return output\n",
    "\n",
    "def bulk_assessment(X,y,metadata,used_model,label_position):\n",
    "    df_residual = []\n",
    "    df_confusion = []\n",
    "    n = len(metadata)\n",
    "    for i in range(0,n):\n",
    "        X_pred = X[i:i+1]\n",
    "        y_true = y[i:i+1]\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            y_pred = used_model(X_pred)\n",
    "        series = metadata.iloc[i]\n",
    "        residual_output = residual(y_pred=y_pred,y_true=y_true,series=series,label_position=label_position)\n",
    "        confusion_output = confusion(y_pred=y_pred,y_true=y_true,label_position=label_position)\n",
    "        residual_output['metadata_index'] = i\n",
    "        residual_output['dataset_index'] = series.dataset_index\n",
    "        df_residual.append(residual_output)\n",
    "        df_confusion.append(confusion_output)\n",
    "        print(f\"\\rData Number {i+1} Processed From {n} Total Event | {(((i+1)/(n))*100):.2f} %\",end=' ')\n",
    "    df_residual = pd.DataFrame(df_residual)\n",
    "    df_confusion = pd.DataFrame(df_confusion)\n",
    "    return df_residual,df_confusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f57c6cb-17b7-4ba6-a13e-6310cdcc51fc",
   "metadata": {},
   "source": [
    "# Folder Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1224fc8-5946-4e9c-914d-5ae854a29b28",
   "metadata": {},
   "source": [
    "- please fill pretrain, sampling_rate, and label_train carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a5332f-e79f-4f1f-ae64-554ea25b6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain = 'stead'\n",
    "sampling_rate = 20\n",
    "label_type = 'gaussian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404b7068-36b8-4652-97c1-ef1a7c9c5d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = r\"C:\\Users\\bonda\\Documents\\Bondan\\TEWS Testing\\dataset\\2025 Januray Picking Dataset 20 Hz\"\n",
    "output_path = r\"C:\\Users\\bonda\\Documents\\Bondan\\TEWS Testing\\model_collection\\model_collection\\PhaseNet Seisbench Jan 2025\"\n",
    "foldername = join_by(['fine','tuning',pretrain,str(sampling_rate)+'Hz',label_type],separator='_')\n",
    "output_folder = join_by([output_path,foldername],separator='\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0c3e29-a29e-4c2e-9c6f-5ec178639491",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(join_by([output_folder,'metadata'],separator='\\\\'), exist_ok=True)\n",
    "os.makedirs(join_by([output_folder,'model'],separator='\\\\'), exist_ok=True)\n",
    "os.makedirs(join_by([output_folder,'performance'],separator='\\\\'), exist_ok=True)\n",
    "os.makedirs(join_by([output_folder,'picture'],separator='\\\\'), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fe4021-72a6-4e1f-9a02-3fcfccaf771b",
   "metadata": {},
   "source": [
    "# Order Setting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1f3e6-7f84-4cf1-b6ab-1645a8ce9bdd",
   "metadata": {},
   "source": [
    "### 1. First Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba654b7e-d6c4-41af-8da3-4eaac2aaee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_order = np.load(join_by([input_folder,'metadata','original_order.npy'],separator='\\\\'),allow_pickle=True).item()\n",
    "original_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1f79f-c3df-475a-9a50-630cc2630c05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = Client(\"GFZ\")\n",
    "t = UTCDateTime(\"2007/01/02 05:48:50\")\n",
    "stream = client.get_waveforms(network=\"CX\", station=\"PB01\", location=\"*\", channel=\"HH?\", starttime=t-100, endtime=t+100)\n",
    "inv = client.get_stations(level='response',network='CX',station='PB01')\n",
    "stream.remove_response(inventory=inv)\n",
    "stream.detrend(type='demean')\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "for i in range(3):\n",
    "    ax.plot(stream[i].times(), stream[i].data, label=stream[i].stats.channel)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bf625f-9a52-41bc-823b-45e1f2fefaa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stn = stream.copy()\n",
    "waveform_duration = stn[0].stats.endtime-stn[0].stats.starttime\n",
    "stn.resample(sampling_rate=(3001/waveform_duration))\n",
    "print(stn)\n",
    "print(stn.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397af4a-d889-41d2-aba4-4cdc272a7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = sbm.PhaseNet()\n",
    "sb_pt_model = model_original.from_pretrained(pretrain)\n",
    "model_original.load_state_dict(sb_pt_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154f680c-8dfb-43d0-8632-a3c210677cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "(model_original.in_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82933622-5f0b-45ae-a2da-ec1b2266635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_component = {0:model_original.component_order[0],1:model_original.component_order[1],2:model_original.component_order[2]}\n",
    "target_component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de5d99-209e-4489-84c0-a01f49544298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_trial = torch.zeros(1,3,3001)\n",
    "X_trial[0,0] = torch.FloatTensor(stn.select(component=target_component[0])[0].data)\n",
    "X_trial[0,1] = torch.FloatTensor(stn.select(component=target_component[1])[0].data)\n",
    "X_trial[0,2] = torch.FloatTensor(stn.select(component=target_component[2])[0].data)\n",
    "min_vals = X_trial.min(dim=-1, keepdim=True).values  \n",
    "max_vals = X_trial.max(dim=-1, keepdim=True).values \n",
    "X_trial = 2 * (X_trial - min_vals) / (max_vals - min_vals) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1bd4d-479b-4659-ae95-d079b25be2dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model_original.eval()\n",
    "    y_pred = model_original(X_trial)\n",
    "fig, axs = plt.subplots(2,figsize=(17,10))\n",
    "axs[0].plot(X_trial[0,0].numpy(),label='0')\n",
    "axs[0].plot(X_trial[0,1].numpy(),label='1')\n",
    "axs[0].plot(X_trial[0,2].numpy(),label='2')\n",
    "axs[0].set_xlabel('Data Point/Time')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "axs[0].set_title('Waveform Data')\n",
    "axs[0].legend()\n",
    "axs[1].plot(y_pred[0,0].numpy(),label='0')\n",
    "axs[1].plot(y_pred[0,1].numpy(),label='1')\n",
    "axs[1].plot(y_pred[0,2].numpy(),label='2')\n",
    "axs[1].set_xlabel('Data Point/Time')\n",
    "axs[1].set_ylabel('Probability')\n",
    "axs[1].set_title('Prediction')\n",
    "axs[1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f6add-3942-4435-8fbc-fea58e017be3",
   "metadata": {},
   "source": [
    "- please determine carefully\n",
    "- when we used the model to predict, sometimes what stored in model_original.labels and actual prediction is not same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b959b4-4083-4f8a-98ae-b313e66aa79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4a76c7-a428-452c-87eb-c754b3d7366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_label = {0:'P',1:'S',2:'N'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733405f3-1288-4c17-a787-cbc482578f1c",
   "metadata": {},
   "source": [
    "### 2. Second Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c7a26-c5ed-4cea-a191-60b9513594a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_summary = pd.read_csv(join_by([input_folder,'metadata','clenaed_metadata_summary.csv'],separator='\\\\'))\n",
    "metadata_summary.index = ['count','mean','std','min','25%','50%','75%','max']    \n",
    "metadata_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe4da8d-d948-4f51-b477-ba88626f978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_comp = {\n",
    "    0: find_index_list(value=target_component[0],value_list=[original_order['data'][i] for i in range(0,3)]),\n",
    "    1: find_index_list(value=target_component[1],value_list=[original_order['data'][i] for i in range(0,3)]),\n",
    "    2: find_index_list(value=target_component[2],value_list=[original_order['data'][i] for i in range(0,3)])\n",
    "}\n",
    "syn_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe19d13-bdca-4221-bf33-ffd9c941c41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_label = {\n",
    "    0: find_index_list(value=target_label[0],value_list=[original_order['label'][i] for i in range(0,3)]),\n",
    "    1: find_index_list(value=target_label[1],value_list=[original_order['label'][i] for i in range(0,3)]),\n",
    "    2: find_index_list(value=target_label[2],value_list=[original_order['label'][i] for i in range(0,3)])\n",
    "}\n",
    "syn_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32508c92-e3ae-4c06-a06f-5b347e154dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "used_model = model_original\n",
    "index = 15139\n",
    "X_trial = torch.zeros(1,3,3001)\n",
    "y_trial = torch.zeros(1,3,3001)\n",
    "#\n",
    "path_in_loop = join_by([input_folder,'data',str(index)+'.pth'],separator='\\\\')\n",
    "data_in_loop = torch.load(path_in_loop)\n",
    "rearrange = torch.zeros(1,3,3001)\n",
    "rearrange[0,0] = data_in_loop[0,syn_comp[0]]\n",
    "rearrange[0,1] = data_in_loop[0,syn_comp[1]]\n",
    "rearrange[0,2] = data_in_loop[0,syn_comp[2]]\n",
    "X_trial[0] = rearrange[0]\n",
    "#\n",
    "min_vals = X_trial.min(dim=-1, keepdim=True).values  \n",
    "max_vals = X_trial.max(dim=-1, keepdim=True).values \n",
    "X_trial = 2 * (X_trial - min_vals) / (max_vals - min_vals) - 1\n",
    "#\n",
    "path_in_loop = join_by([input_folder,'label',str(index)+'.pth'],separator='\\\\')\n",
    "label_in_loop = torch.load(path_in_loop)\n",
    "rearrange = torch.zeros(1,3,3001)\n",
    "rearrange[0,0] = label_in_loop[0,syn_label[0]]\n",
    "rearrange[0,1] = label_in_loop[0,syn_label[1]]\n",
    "rearrange[0,2] = label_in_loop[0,syn_label[2]]\n",
    "y_trial[0] = rearrange[0]\n",
    "#\n",
    "with torch.no_grad():\n",
    "    used_model.eval()\n",
    "    y_pred = used_model(X_trial)\n",
    "fig, axs = plt.subplots(2,figsize=(17,10))\n",
    "axs[0].plot(X_trial[0,0].numpy(),label='0')\n",
    "axs[0].plot(X_trial[0,1].numpy(),label='1')\n",
    "axs[0].plot(X_trial[0,2].numpy(),label='2')\n",
    "axs[0].set_xlabel('Data Point/Time')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "axs[0].set_title('Waveform Data')\n",
    "axs[0].legend()\n",
    "axs[1].plot(y_pred[0,0].numpy(),label='0')\n",
    "axs[1].plot(y_pred[0,1].numpy(),label='1')\n",
    "axs[1].plot(y_pred[0,2].numpy(),label='2')\n",
    "axs[1].set_xlabel('Data Point/Time')\n",
    "axs[1].set_ylabel('Probability')\n",
    "axs[1].set_title('Prediction')\n",
    "axs[1].legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c395e7-a196-4ebd-b9c8-1572a00a879f",
   "metadata": {},
   "source": [
    "# Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5633199-185d-4aa0-b9b7-3ce777d67747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(join_by([input_folder,'metadata','cleaned_metadata.csv'],separator='\\\\'))\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0356e8-8d64-4ecc-aa0f-4ed08e255b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = np.int64(metadata[metadata['splitting'] == 'train']['dataset_index'].tolist())\n",
    "test_index = np.int64(metadata[metadata['splitting'] == 'test']['dataset_index'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060380ed-b056-4000-9191-77534c6c4ccc",
   "metadata": {},
   "source": [
    "# Load Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5cc9c-8bf8-48c4-aadd-7e051c1c4516",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_train_index = copy.deepcopy(train_index)\n",
    "real_train_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bb12c8-a625-43d6-a821-cc43a8662532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = torch.zeros(len(train_index),3,3001)\n",
    "y_train = torch.zeros(len(train_index),3,3001)\n",
    "for i in range(0,len(train_index)):\n",
    "    #\n",
    "    path_in_loop = join_by([input_folder,'data',str(train_index[i])+'.pth'],separator='\\\\')\n",
    "    data_in_loop = torch.load(path_in_loop)\n",
    "    rearrange = torch.zeros(1,3,3001)\n",
    "    rearrange[0,0] = data_in_loop[0,syn_comp[0]]\n",
    "    rearrange[0,1] = data_in_loop[0,syn_comp[1]]\n",
    "    rearrange[0,2] = data_in_loop[0,syn_comp[2]]\n",
    "    X_train[i] = rearrange[0]\n",
    "    #\n",
    "    path_in_loop = join_by([input_folder,'label',str(train_index[i])+'.pth'],separator='\\\\')\n",
    "    label_in_loop = torch.load(path_in_loop)\n",
    "    rearrange = torch.zeros(1,3,3001)\n",
    "    rearrange[0,0] = label_in_loop[0,syn_label[0]]\n",
    "    rearrange[0,1] = label_in_loop[0,syn_label[1]]\n",
    "    rearrange[0,2] = label_in_loop[0,syn_label[2]]\n",
    "    y_train[i] = rearrange[0]\n",
    "    #\n",
    "    print(f\"\\rData Number {i+1} Loaded From {len(train_index)} Total Data Available | {(((i+1)/(len(train_index)))*100):.2f} % {''*200}\",end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967dc612-28cc-4f41-998e-b891c45d131f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_indices = find_nan_with_data_index(X_train)\n",
    "data_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294f27c6-d87c-4c8b-a2d2-72d98f8a07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a9276b-d164-46c4-8504-cd21767359e0",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5fcab-15ab-45a0-b496-4baeaa69719c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_test_index = copy.deepcopy(test_index)\n",
    "real_test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a68c9-1d9f-45ec-bfe2-ff03559eb5a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test = torch.zeros(len(test_index),3,3001)\n",
    "y_test = torch.zeros(len(test_index),3,3001)\n",
    "for i in range(0,len(test_index)):\n",
    "    #\n",
    "    path_in_loop = join_by([input_folder,'data',str(test_index[i])+'.pth'],separator='\\\\')\n",
    "    data_in_loop = torch.load(path_in_loop)\n",
    "    rearrange = torch.zeros(1,3,3001)\n",
    "    rearrange[0,0] = data_in_loop[0,syn_comp[0]]\n",
    "    rearrange[0,1] = data_in_loop[0,syn_comp[1]]\n",
    "    rearrange[0,2] = data_in_loop[0,syn_comp[2]]\n",
    "    X_test[i] = rearrange[0]\n",
    "    #\n",
    "    path_in_loop = join_by([input_folder,'label',str(test_index[i])+'.pth'],separator='\\\\')\n",
    "    label_in_loop = torch.load(path_in_loop)\n",
    "    rearrange = torch.zeros(1,3,3001)\n",
    "    rearrange[0,0] = label_in_loop[0,syn_label[0]]\n",
    "    rearrange[0,1] = label_in_loop[0,syn_label[1]]\n",
    "    rearrange[0,2] = label_in_loop[0,syn_label[2]]\n",
    "    y_test[i] = rearrange[0]\n",
    "    #\n",
    "    print(f\"\\rData Number {i+1} Loaded From {len(test_index)} Total Data Available | {(((i+1)/(len(test_index)))*100):.2f} % {''*200}\",end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac6a385-751f-42e5-a2be-437e0b330748",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_indices = find_nan_with_data_index(X_test)\n",
    "data_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529d25b8-ca77-437c-b08a-24b338e56857",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93306bdd-9562-4871-95a2-afa3d7919802",
   "metadata": {},
   "source": [
    "# Data Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e4c71-22ad-4f56-8531-daefebf2fcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vals = X_train.min(dim=-1, keepdim=True).values \n",
    "max_vals = X_train.max(dim=-1, keepdim=True).values\n",
    "X_train = 2 * (X_train - min_vals) / (max_vals - min_vals) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58df5d-0be3-468a-801a-a34f8333bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595a94d-943c-42da-a8b1-764d537868d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_vals = X_test.min(dim=-1, keepdim=True).values  \n",
    "max_vals = X_test.max(dim=-1, keepdim=True).values \n",
    "X_test = 2 * (X_test - min_vals) / (max_vals - min_vals) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68cf9a9-aea2-4f96-80fd-c263e418276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8d2ea-05bc-48d0-875f-9b4b04cb9f24",
   "metadata": {},
   "source": [
    "# Tensor Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec90d14-1dbb-44b5-8e67-59a68f5b519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 8\n",
    "train_set = TensorDataset(X_train,y_train)\n",
    "train_loader = DataLoader(train_set,batch_size=bs, shuffle=True)\n",
    "test_set = TensorDataset(X_test,y_test)\n",
    "test_loader = DataLoader(test_set,batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c0fc8-3022-4a50-bd79-9907ca97debc",
   "metadata": {},
   "source": [
    "# Load Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea6783-8848-4a50-b35a-f7d3fa398e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_original = sbm.PhaseNet()\n",
    "sb_pt_model = model_original.from_pretrained(pretrain)\n",
    "model_original.load_state_dict(sb_pt_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9239c186-28b5-45d7-a7ed-8b11293d407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = copy.deepcopy(model_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9bc62d-0a1a-40ba-83a3-78cf5ccad0a4",
   "metadata": {},
   "source": [
    "# Set Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63110c7-cd3d-4007-9999-f1e51c1f2075",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f08e9-a60d-4c28-974d-986517f4e8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = 1e-3\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534877f-dd18-4be7-9ef2-a27b29b4fe9d",
   "metadata": {},
   "source": [
    "# Set Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aede3aa9-cb3e-43e8-a6eb-b7348665729f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(y_pred,y_true,eps=1e-5):\n",
    "    h = y_true * torch.log(y_pred + eps)\n",
    "    h_shape = h.shape\n",
    "    for i in range(0,h_shape[0]):\n",
    "        for j in range(0,h_shape[1]):\n",
    "            nan_list = find_nan_with_data_index(h[i,j])\n",
    "            if len(nan_list) != 0:\n",
    "                tensor = h[i,j]\n",
    "                clean_tensor = tensor[~torch.isnan(tensor)]\n",
    "                min_value = clean_tensor.min()\n",
    "            for k in nan_list:\n",
    "                h[i,j,k] = min_value\n",
    "    h = h.mean(-1).sum(-1)  # Mean along sample dimension and sum along pick dimension\n",
    "    h = h.mean()  # Mean over batch axis\n",
    "    return -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7e7267-1a6f-4193-aaa2-4ff163cd1e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_alpha_beta(data):\n",
    "    # Flatten all dimensions except the batch\n",
    "    flattened_data = data.view(-1)  # Shape: (N * C * D)    \n",
    "    # Count positive and negative samples\n",
    "    num_pos = (flattened_data == 1).sum().item()\n",
    "    num_neg = (flattened_data == 0).sum().item()\n",
    "    total = num_pos + num_neg\n",
    "    if total == 0:\n",
    "        raise ValueError(\"No valid samples in the input data.\")\n",
    "    alpha = num_neg / total\n",
    "    beta = num_pos / total\n",
    "    return alpha,beta\n",
    "\n",
    "def criterion(predictions, targets, alpha, beta):\n",
    "    # Add a small epsilon to prevent log(0)\n",
    "    epsilon = 1e-8\n",
    "    loss = -(\n",
    "        alpha * targets * torch.log(predictions + epsilon) +\n",
    "        beta * (1 - targets) * torch.log(1 - predictions + epsilon)\n",
    "    )\n",
    "    return loss.mean()\n",
    "\n",
    "alpha, beta = calculate_alpha_beta(y_train)\n",
    "print(f\"Alpha: {alpha}, Beta: {beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6a05a-f416-4e79-8921-1bf44c26a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cea62d5-2e32-4754-bae7-a70c0e85fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion(y_train[0:1],y_train[0:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15201fa1-0060-423c-85d7-96dd1aa8dab1",
   "metadata": {},
   "source": [
    "# Checking Again Train dan Test Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73675c0-1a37-4914-98c0-fc64e775f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217c0ca7-d792-4200-addc-42debe4e79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f7e93f-050a-4513-80e3-e1be3d01ff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 10\n",
    "fig, axs = plt.subplots(2,figsize=(17,10))\n",
    "axs[0].plot(X_train[index,0].numpy(),label='Z')\n",
    "axs[0].plot(X_train[index,1].numpy(),label='N')\n",
    "axs[0].plot(X_train[index,2].numpy(),label='E')\n",
    "axs[0].set_xlabel('Data Point/Time')\n",
    "axs[0].set_ylabel('Amplitude')\n",
    "axs[0].set_title('Waveform Data')\n",
    "axs[0].legend()\n",
    "axs[1].plot(y_train[index,0].numpy(),label='0')\n",
    "axs[1].plot(y_train[index,1].numpy(),label='1')\n",
    "axs[1].plot(y_train[index,2].numpy(),label='2')\n",
    "axs[1].set_xlabel('Data Point/Time')\n",
    "axs[1].set_ylabel('Amplitude')\n",
    "axs[1].set_title('Label Data')\n",
    "axs[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb9dd7-264a-42a6-a19e-caf4f4fe4b92",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4dc52d-481d-4244-b58b-3987533c3107",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def str_zero(x):\n",
    "    if x < 10:\n",
    "        str1 = '  ' + str(int(x)) + ' %'\n",
    "    elif x >= 10 and x < 100:\n",
    "        str1 = ' ' + str(int(x)) + ' %'\n",
    "    elif x >= 100:\n",
    "        str1 = str(int(x)) + ' %'\n",
    "    return str1\n",
    "\n",
    "weight_saving_index = 0\n",
    "n = len(train_loader) + len(test_loader)\n",
    "model_container = []\n",
    "epochs = 100\n",
    "train_cost, test_cost, time_run = [0], [0], []\n",
    "run_time = 0\n",
    "\n",
    "for i in range(epochs):\n",
    "    st = time.time()\n",
    "    count = 0\n",
    "    \n",
    "    # data train\n",
    "    model.train()\n",
    "    cost = 0\n",
    "    train_count = 0\n",
    "    for feature,target in train_loader:\n",
    "        \n",
    "        output = model(feature)\n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cost += loss.item() * feature.shape[0]\n",
    "        train_count += 1\n",
    "        count += 1\n",
    "        \n",
    "        print(f\"\\rEpoch: {i+1:4}/{epochs} [{str_zero((count/n)*100)}] | train_cost {train_cost[-1]:.7f} ({loss.item()}) | test_cost {test_cost[-1]:.7f} () | time {run_time:.5f} s\", end=' ')\n",
    "    \n",
    "    train_cost.append(cost / len(train_set))\n",
    "    \n",
    "    # data test\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        cost = 0\n",
    "        test_count = 0\n",
    "        for feature,target in test_loader:\n",
    "            \n",
    "            output = model(feature)\n",
    "            loss = criterion(output,target)\n",
    "            \n",
    "            cost += loss.item() * feature.shape[0]\n",
    "            test_count += 1\n",
    "            count += 1\n",
    "            \n",
    "            print(f\"\\rEpoch: {i+1:4}/{epochs} [{str_zero((count/n)*100)}] | train_cost {train_cost[-1]:.7f} () | test_cost {test_cost[-1]:.7f} ({loss.item()}) | time {run_time:.5f} s\", end=' ')\n",
    "        \n",
    "        test_cost.append(cost / len(test_set))\n",
    "    \n",
    "    # print report\n",
    "    torch.save(model.state_dict(),join_by([output_folder,'model',\"weights_epoch_\" + str(weight_saving_index) + \".pth\"],separator='\\\\'))\n",
    "    weight_saving_index += 1\n",
    "    et = time.time()\n",
    "    run_time = et - st\n",
    "    time_run.append(run_time)\n",
    "    \n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d773cec7-ad7d-403d-9120-b7c77098f9d8",
   "metadata": {},
   "source": [
    "# Loss Function Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ab75c-6fc1-4b19-9712-6a83cadf2b30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_cost[1:len(train_cost)],label='Data Train')\n",
    "plt.plot(test_cost[1:len(test_cost)],label='Data Test')\n",
    "plt.title('Loss Function Curve As Function of Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.legend()\n",
    "plt.savefig(join_by([output_folder,'picture',\"loss_curve.png\"],separator='\\\\'), dpi=2000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a8a9dd-4ff5-4b0b-86a2-25a881ca969f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_metadata = pd.DataFrame()\n",
    "loss_metadata['loss_train'] = train_cost[1:len(train_cost)]\n",
    "loss_metadata['loss_test'] = test_cost[1:len(test_cost)]\n",
    "loss_metadata['run_time'] = time_run\n",
    "loss_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92669e7e-eb15-4408-886f-1252962d77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_metadata.to_csv(join_by([output_folder,'metadata',\"loss_metadata.csv\"],separator='\\\\'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270a1b4-515b-48e4-a965-cc755a42d7ca",
   "metadata": {},
   "source": [
    "# Residual Pick and Confusion Matrix Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b10ad3-059f-40ee-9495-5bb682302157",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_position = {'P': 0,'S': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ead0063-d708-405a-b99f-e0d0d539beee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_metadata = metadata[metadata['splitting'] == 'train'].copy()\n",
    "train_metadata.index = [i for i in range(0,len(train_metadata))]\n",
    "train_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32026a-4d5f-43e1-b4ea-de48084b84f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metadata = metadata[metadata['splitting'] == 'test'].copy()\n",
    "test_metadata.index = [i for i in range(0,len(test_metadata))]\n",
    "test_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2cb018-04b0-458b-b4bb-a4d1f02f2a23",
   "metadata": {},
   "source": [
    "### 1. After (Data Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6167172-4fbc-46c5-9d36-a1588df27d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_train,df_confusion_train = bulk_assessment(X=X_train,y=y_train,metadata=train_metadata,used_model=model,label_position=label_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef8fda0-5b93-492f-a049-ba52652ca3df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_residual_train_concat = pd.concat([df_residual_train,df_confusion_train,train_metadata],axis=1)\n",
    "df_residual_train_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435b7c3b-5eca-40a7-b63c-a930be083e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_train_concat.to_csv(join_by([output_folder,'performance',\"train_performance_after.csv\"],separator='\\\\'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33589305-c6be-45e1-9c1e-3a0e34d22cb8",
   "metadata": {},
   "source": [
    "### 2. After (Data Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d15c3e-1922-4479-8e61-28cab5c2bc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_test,df_confusion_test = bulk_assessment(X=X_test,y=y_test,metadata=test_metadata,used_model=model,label_position=label_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c967d9e2-7b99-44b9-bfa6-d115b3c55fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_residual_test_concat = pd.concat([df_residual_test,df_confusion_test,test_metadata],axis=1)\n",
    "df_residual_test_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c866879-34d4-40ec-846b-b405f70f600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_test_concat.to_csv(join_by([output_folder,'performance',\"test_performance_after.csv\"],separator='\\\\'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90fb64-1d6f-42cb-af4d-9b6d79e49de4",
   "metadata": {},
   "source": [
    "### 3. Before (Data Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99823483-2747-4f03-8a90-fcfb66d4c62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_train,df_confusion_train = bulk_assessment(X=X_train,y=y_train,metadata=train_metadata,used_model=model_original,label_position=label_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d397c61d-a839-4a6e-957f-72af370a9c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_train_concat = pd.concat([df_residual_train,df_confusion_train,train_metadata],axis=1)\n",
    "df_residual_train_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27cbbb9-214d-4753-93fa-c7edbb9831d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_train_concat.to_csv(join_by([output_folder,'performance',\"train_performance_before.csv\"],separator='\\\\'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898cb4ed-e758-4d2e-81ca-70d047e02853",
   "metadata": {},
   "source": [
    "### 4. Before (Data Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2574057c-2d70-4140-b182-57d0059cedb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_test,df_confusion_test = bulk_assessment(X=X_test,y=y_test,metadata=test_metadata,used_model=model_original,label_position=label_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78125a-0f87-46f6-af5f-5b2e3349afb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_test_concat = pd.concat([df_residual_test,df_confusion_test,test_metadata],axis=1)\n",
    "df_residual_test_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121e417a-ed08-4a65-8739-c65c5938baeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_residual_test_concat.to_csv(join_by([output_folder,'performance',\"test_performance_before.csv\"],separator='\\\\'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ed7b6a-9e3d-465a-adf7-f0390b959a69",
   "metadata": {},
   "source": [
    "# Model Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0f0fed-c9be-4092-a917-1e8242d51a7e",
   "metadata": {},
   "source": [
    "- please fill carefully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db80e3fa-7488-406f-8026-f5b5ebeb5847",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_metadata = pd.DataFrame({\n",
    "    'model':['PhaseNet'],\n",
    "    'pretrain':[pretrain],\n",
    "    'sampling_rate':[sampling_rate],\n",
    "    'data_order': ['ZNE'],\n",
    "    'label_order': ['PSN'],\n",
    "    'window_duration':[150],\n",
    "    'optimizer':['Adam'],\n",
    "    'learning_rate':[learning_rate],\n",
    "    'weight_decay':[False],\n",
    "    'best_weight': [12],\n",
    "    'loss_function': ['custom'],\n",
    "    'scaling': ['-1/1 scaling']\n",
    "})\n",
    "model_metadata = model_metadata.T\n",
    "model_metadata['param'] = model_metadata.index\n",
    "model_metadata['value'] = model_metadata[0]\n",
    "model_metadata.drop(columns=[0],inplace=True)\n",
    "model_metadata.index = [i for i in range(0,len(model_metadata))]\n",
    "model_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad56da10-9e36-4419-8a3c-61f6d26f59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata.to_csv(join_by([output_folder,'metadata',\"model_metadata.csv\"],separator='\\\\'),index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
